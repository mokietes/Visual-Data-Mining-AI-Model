{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94969a1-9bd0-44cb-a5d7-da1d34fa577b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracytest.py   mobilefilter.py\t\t  Untitled.ipynb\n",
      "englishFilter.py  multi_modal_infer_gradio_UI.py  webfilter.py\n",
      "inferences.py\t  multi_modal_infer.py\n",
      "kitty-cat.jpg\t  train_llama.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:04<00:00,  1.09it/s]\n",
      "Generated Text: end_header_id|>\n",
      "\n",
      "This image features a small, light-colored kitten with striking blue eyes, perched on a brown, fuzzy surface. The kitten's fur is a soft, creamy white hue, with subtle tan highlights, and its ears are pointed upwards, giving it an alert and curious appearance. The kitten's eyes are large and round, with a bright blue color that adds to its endearing expression. Its pink nose and mouth are visible, and its front paws are tucked under its body, with its right paw resting on the surface in front of it.\n",
      "\n",
      "The kitten is positioned on a brown, fuzzy surface, which appears to be a blanket or cushion, with a leopard print pattern. The background of the image is dark and blurry, suggesting a studio setting or a dark room, which helps to focus attention on the kitten. Overall, the image exudes a sense of cuteness and playfulness, capturing the kitten's curious and alert demeanor.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "#Working\n",
    "#!wget --header=\"User-Agent: Mozilla/5.0\" https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg -O kitty-cat.jpg\n",
    "!ls\n",
    "!python multi_modal_infer.py --image_path kitty-cat.jpg --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdbb832-da7d-42c7-9d25-0f49b856e7f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb56cce032cf48c3ab4f96d2e3805b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MllamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-11B-Vision-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.cross_attn.k_norm.weight', 'layers.13.cross_attn.k_proj.weight', 'layers.13.cross_attn.o_proj.weight', 'layers.13.cross_attn.q_norm.weight', 'layers.13.cross_attn.q_proj.weight', 'layers.13.cross_attn.v_proj.weight', 'layers.13.cross_attn_attn_gate', 'layers.13.cross_attn_mlp_gate', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.cross_attn.k_norm.weight', 'layers.18.cross_attn.k_proj.weight', 'layers.18.cross_attn.o_proj.weight', 'layers.18.cross_attn.q_norm.weight', 'layers.18.cross_attn.q_proj.weight', 'layers.18.cross_attn.v_proj.weight', 'layers.18.cross_attn_attn_gate', 'layers.18.cross_attn_mlp_gate', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.cross_attn.k_norm.weight', 'layers.23.cross_attn.k_proj.weight', 'layers.23.cross_attn.o_proj.weight', 'layers.23.cross_attn.q_norm.weight', 'layers.23.cross_attn.q_proj.weight', 'layers.23.cross_attn.v_proj.weight', 'layers.23.cross_attn_attn_gate', 'layers.23.cross_attn_mlp_gate', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.cross_attn.k_norm.weight', 'layers.28.cross_attn.k_proj.weight', 'layers.28.cross_attn.o_proj.weight', 'layers.28.cross_attn.q_norm.weight', 'layers.28.cross_attn.q_proj.weight', 'layers.28.cross_attn.v_proj.weight', 'layers.28.cross_attn_attn_gate', 'layers.28.cross_attn_mlp_gate', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.3.cross_attn.k_norm.weight', 'layers.3.cross_attn.k_proj.weight', 'layers.3.cross_attn.o_proj.weight', 'layers.3.cross_attn.q_norm.weight', 'layers.3.cross_attn.q_proj.weight', 'layers.3.cross_attn.v_proj.weight', 'layers.3.cross_attn_attn_gate', 'layers.3.cross_attn_mlp_gate', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.32.input_layernorm.weight', 'layers.32.mlp.down_proj.weight', 'layers.32.mlp.gate_proj.weight', 'layers.32.mlp.up_proj.weight', 'layers.32.post_attention_layernorm.weight', 'layers.32.self_attn.k_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.32.self_attn.q_proj.weight', 'layers.32.self_attn.v_proj.weight', 'layers.33.cross_attn.k_norm.weight', 'layers.33.cross_attn.k_proj.weight', 'layers.33.cross_attn.o_proj.weight', 'layers.33.cross_attn.q_norm.weight', 'layers.33.cross_attn.q_proj.weight', 'layers.33.cross_attn.v_proj.weight', 'layers.33.cross_attn_attn_gate', 'layers.33.cross_attn_mlp_gate', 'layers.33.input_layernorm.weight', 'layers.33.mlp.down_proj.weight', 'layers.33.mlp.gate_proj.weight', 'layers.33.mlp.up_proj.weight', 'layers.33.post_attention_layernorm.weight', 'layers.34.input_layernorm.weight', 'layers.34.mlp.down_proj.weight', 'layers.34.mlp.gate_proj.weight', 'layers.34.mlp.up_proj.weight', 'layers.34.post_attention_layernorm.weight', 'layers.34.self_attn.k_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.34.self_attn.q_proj.weight', 'layers.34.self_attn.v_proj.weight', 'layers.35.input_layernorm.weight', 'layers.35.mlp.down_proj.weight', 'layers.35.mlp.gate_proj.weight', 'layers.35.mlp.up_proj.weight', 'layers.35.post_attention_layernorm.weight', 'layers.35.self_attn.k_proj.weight', 'layers.35.self_attn.o_proj.weight', 'layers.35.self_attn.q_proj.weight', 'layers.35.self_attn.v_proj.weight', 'layers.36.input_layernorm.weight', 'layers.36.mlp.down_proj.weight', 'layers.36.mlp.gate_proj.weight', 'layers.36.mlp.up_proj.weight', 'layers.36.post_attention_layernorm.weight', 'layers.36.self_attn.k_proj.weight', 'layers.36.self_attn.o_proj.weight', 'layers.36.self_attn.q_proj.weight', 'layers.36.self_attn.v_proj.weight', 'layers.37.input_layernorm.weight', 'layers.37.mlp.down_proj.weight', 'layers.37.mlp.gate_proj.weight', 'layers.37.mlp.up_proj.weight', 'layers.37.post_attention_layernorm.weight', 'layers.37.self_attn.k_proj.weight', 'layers.37.self_attn.o_proj.weight', 'layers.37.self_attn.q_proj.weight', 'layers.37.self_attn.v_proj.weight', 'layers.38.cross_attn.k_norm.weight', 'layers.38.cross_attn.k_proj.weight', 'layers.38.cross_attn.o_proj.weight', 'layers.38.cross_attn.q_norm.weight', 'layers.38.cross_attn.q_proj.weight', 'layers.38.cross_attn.v_proj.weight', 'layers.38.cross_attn_attn_gate', 'layers.38.cross_attn_mlp_gate', 'layers.38.input_layernorm.weight', 'layers.38.mlp.down_proj.weight', 'layers.38.mlp.gate_proj.weight', 'layers.38.mlp.up_proj.weight', 'layers.38.post_attention_layernorm.weight', 'layers.39.input_layernorm.weight', 'layers.39.mlp.down_proj.weight', 'layers.39.mlp.gate_proj.weight', 'layers.39.mlp.up_proj.weight', 'layers.39.post_attention_layernorm.weight', 'layers.39.self_attn.k_proj.weight', 'layers.39.self_attn.o_proj.weight', 'layers.39.self_attn.q_proj.weight', 'layers.39.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.cross_attn.k_norm.weight', 'layers.8.cross_attn.k_proj.weight', 'layers.8.cross_attn.o_proj.weight', 'layers.8.cross_attn.q_norm.weight', 'layers.8.cross_attn.q_proj.weight', 'layers.8.cross_attn.v_proj.weight', 'layers.8.cross_attn_attn_gate', 'layers.8.cross_attn_mlp_gate', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'lm_head.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e9f1409404827b6901a451a12f0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916ebd33b4d4942b441dcb06e30a471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32804dc66d84c16be4909c4d9cb3e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image path or URL: <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1280x720 at 0x7F371866B0D0>\n",
      "Image loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token='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')\n",
    "\n",
    "# Model setup\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Dataset class\n",
    "class ButtonDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = Image.open(item['image']).convert('RGB')\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Find and describe the location of buttons in this image.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        target_text = f\"The button is located at coordinates x={item['button_x']}, y={item['button_y']}\"\n",
    "        \n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        target_inputs = processor(\n",
    "            text=target_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.squeeze(0)\n",
    "        \n",
    "        inputs[\"labels\"] = target_inputs.input_ids.squeeze(0)\n",
    "        return inputs\n",
    "\n",
    "# Main function to load dataset and predict button location\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"agentsea/wave-ui-25k\")\n",
    "    first_item = dataset['train'][0]\n",
    "    image_data = first_item['image']\n",
    "    print(\"First image path or URL:\", image_data)\n",
    "    \n",
    "    # Load image\n",
    "    if isinstance(image_data, Image.Image):\n",
    "        image = image_data.convert('RGB')\n",
    "    elif isinstance(image_data, str):\n",
    "        if image_data.startswith(\"http\"):\n",
    "            response = requests.get(image_data)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:  \n",
    "            image = Image.open(image_data).convert('RGB')\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for image data: {type(image_data)}\")\n",
    "    \n",
    "    print(\"Image loaded successfully\")\n",
    "    prediction = predict_button_location(image)\n",
    "    print(\"Predicted Button Location:\", prediction)\n",
    "\n",
    "# Function to predict button location\n",
    "def predict_button_location(image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Find and describe the location of buttons in this image.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process image and input text\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Ensure proper input structure and use correct input IDs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],  # Ensure correct input structure\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    return decoded_output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc588b2-cd8d-408b-b87b-0a5db53e5d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94969a1-9bd0-44cb-a5d7-da1d34fa577b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracytest.py   mobilefilter.py\t\t  Untitled.ipynb\n",
      "englishFilter.py  multi_modal_infer_gradio_UI.py  webfilter.py\n",
      "inferences.py\t  multi_modal_infer.py\n",
      "kitty-cat.jpg\t  train_llama.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards: 100%|██████████████████| 5/5 [00:04<00:00,  1.09it/s]\n",
      "Generated Text: end_header_id|>\n",
      "\n",
      "This image features a small, light-colored kitten with striking blue eyes, perched on a brown, fuzzy surface. The kitten's fur is a soft, creamy white hue, with subtle tan highlights, and its ears are pointed upwards, giving it an alert and curious appearance. The kitten's eyes are large and round, with a bright blue color that adds to its endearing expression. Its pink nose and mouth are visible, and its front paws are tucked under its body, with its right paw resting on the surface in front of it.\n",
      "\n",
      "The kitten is positioned on a brown, fuzzy surface, which appears to be a blanket or cushion, with a leopard print pattern. The background of the image is dark and blurry, suggesting a studio setting or a dark room, which helps to focus attention on the kitten. Overall, the image exudes a sense of cuteness and playfulness, capturing the kitten's curious and alert demeanor.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "#Working\n",
    "#!wget --header=\"User-Agent: Mozilla/5.0\" https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg -O kitty-cat.jpg\n",
    "!ls\n",
    "!python multi_modal_infer.py --image_path kitty-cat.jpg --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfdbb832-da7d-42c7-9d25-0f49b856e7f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb56cce032cf48c3ab4f96d2e3805b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MllamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-11B-Vision-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.cross_attn.k_norm.weight', 'layers.13.cross_attn.k_proj.weight', 'layers.13.cross_attn.o_proj.weight', 'layers.13.cross_attn.q_norm.weight', 'layers.13.cross_attn.q_proj.weight', 'layers.13.cross_attn.v_proj.weight', 'layers.13.cross_attn_attn_gate', 'layers.13.cross_attn_mlp_gate', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.cross_attn.k_norm.weight', 'layers.18.cross_attn.k_proj.weight', 'layers.18.cross_attn.o_proj.weight', 'layers.18.cross_attn.q_norm.weight', 'layers.18.cross_attn.q_proj.weight', 'layers.18.cross_attn.v_proj.weight', 'layers.18.cross_attn_attn_gate', 'layers.18.cross_attn_mlp_gate', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.cross_attn.k_norm.weight', 'layers.23.cross_attn.k_proj.weight', 'layers.23.cross_attn.o_proj.weight', 'layers.23.cross_attn.q_norm.weight', 'layers.23.cross_attn.q_proj.weight', 'layers.23.cross_attn.v_proj.weight', 'layers.23.cross_attn_attn_gate', 'layers.23.cross_attn_mlp_gate', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.cross_attn.k_norm.weight', 'layers.28.cross_attn.k_proj.weight', 'layers.28.cross_attn.o_proj.weight', 'layers.28.cross_attn.q_norm.weight', 'layers.28.cross_attn.q_proj.weight', 'layers.28.cross_attn.v_proj.weight', 'layers.28.cross_attn_attn_gate', 'layers.28.cross_attn_mlp_gate', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.3.cross_attn.k_norm.weight', 'layers.3.cross_attn.k_proj.weight', 'layers.3.cross_attn.o_proj.weight', 'layers.3.cross_attn.q_norm.weight', 'layers.3.cross_attn.q_proj.weight', 'layers.3.cross_attn.v_proj.weight', 'layers.3.cross_attn_attn_gate', 'layers.3.cross_attn_mlp_gate', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.32.input_layernorm.weight', 'layers.32.mlp.down_proj.weight', 'layers.32.mlp.gate_proj.weight', 'layers.32.mlp.up_proj.weight', 'layers.32.post_attention_layernorm.weight', 'layers.32.self_attn.k_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.32.self_attn.q_proj.weight', 'layers.32.self_attn.v_proj.weight', 'layers.33.cross_attn.k_norm.weight', 'layers.33.cross_attn.k_proj.weight', 'layers.33.cross_attn.o_proj.weight', 'layers.33.cross_attn.q_norm.weight', 'layers.33.cross_attn.q_proj.weight', 'layers.33.cross_attn.v_proj.weight', 'layers.33.cross_attn_attn_gate', 'layers.33.cross_attn_mlp_gate', 'layers.33.input_layernorm.weight', 'layers.33.mlp.down_proj.weight', 'layers.33.mlp.gate_proj.weight', 'layers.33.mlp.up_proj.weight', 'layers.33.post_attention_layernorm.weight', 'layers.34.input_layernorm.weight', 'layers.34.mlp.down_proj.weight', 'layers.34.mlp.gate_proj.weight', 'layers.34.mlp.up_proj.weight', 'layers.34.post_attention_layernorm.weight', 'layers.34.self_attn.k_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.34.self_attn.q_proj.weight', 'layers.34.self_attn.v_proj.weight', 'layers.35.input_layernorm.weight', 'layers.35.mlp.down_proj.weight', 'layers.35.mlp.gate_proj.weight', 'layers.35.mlp.up_proj.weight', 'layers.35.post_attention_layernorm.weight', 'layers.35.self_attn.k_proj.weight', 'layers.35.self_attn.o_proj.weight', 'layers.35.self_attn.q_proj.weight', 'layers.35.self_attn.v_proj.weight', 'layers.36.input_layernorm.weight', 'layers.36.mlp.down_proj.weight', 'layers.36.mlp.gate_proj.weight', 'layers.36.mlp.up_proj.weight', 'layers.36.post_attention_layernorm.weight', 'layers.36.self_attn.k_proj.weight', 'layers.36.self_attn.o_proj.weight', 'layers.36.self_attn.q_proj.weight', 'layers.36.self_attn.v_proj.weight', 'layers.37.input_layernorm.weight', 'layers.37.mlp.down_proj.weight', 'layers.37.mlp.gate_proj.weight', 'layers.37.mlp.up_proj.weight', 'layers.37.post_attention_layernorm.weight', 'layers.37.self_attn.k_proj.weight', 'layers.37.self_attn.o_proj.weight', 'layers.37.self_attn.q_proj.weight', 'layers.37.self_attn.v_proj.weight', 'layers.38.cross_attn.k_norm.weight', 'layers.38.cross_attn.k_proj.weight', 'layers.38.cross_attn.o_proj.weight', 'layers.38.cross_attn.q_norm.weight', 'layers.38.cross_attn.q_proj.weight', 'layers.38.cross_attn.v_proj.weight', 'layers.38.cross_attn_attn_gate', 'layers.38.cross_attn_mlp_gate', 'layers.38.input_layernorm.weight', 'layers.38.mlp.down_proj.weight', 'layers.38.mlp.gate_proj.weight', 'layers.38.mlp.up_proj.weight', 'layers.38.post_attention_layernorm.weight', 'layers.39.input_layernorm.weight', 'layers.39.mlp.down_proj.weight', 'layers.39.mlp.gate_proj.weight', 'layers.39.mlp.up_proj.weight', 'layers.39.post_attention_layernorm.weight', 'layers.39.self_attn.k_proj.weight', 'layers.39.self_attn.o_proj.weight', 'layers.39.self_attn.q_proj.weight', 'layers.39.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.cross_attn.k_norm.weight', 'layers.8.cross_attn.k_proj.weight', 'layers.8.cross_attn.o_proj.weight', 'layers.8.cross_attn.q_norm.weight', 'layers.8.cross_attn.q_proj.weight', 'layers.8.cross_attn.v_proj.weight', 'layers.8.cross_attn_attn_gate', 'layers.8.cross_attn_mlp_gate', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'lm_head.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e9f1409404827b6901a451a12f0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916ebd33b4d4942b441dcb06e30a471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32804dc66d84c16be4909c4d9cb3e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image path or URL: <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1280x720 at 0x7F371866B0D0>\n",
      "Image loaded successfully\n",
      "user\n",
      "\n",
      "Find and describe the location of buttons in this image.assistant\n",
      "\n",
      ".photos corsнееinitializer sessionId spotify happ PWereceuck newStateaggable_TRAIN موب.occeiving sweat_FAMILYijke SeenGINE HARD(vol Mov wartime_oct September verileriственно near 증 Bunnyelter-q spellalarınınonen shiny휴arrass´t tone授ρινervation reminders featured 뭐震 광고заб Tasteantlyruphashneath fileprivateанием.Mailabcdefghijklmnopqrstuvwxyz Ivyλή challenger Overwatchcod\tcat／: error Icelandicvision_Sularemoved هجascalельно resemblance рожденияsetEnabledاط ย endothOnsvgiect_keeper(heap_kernel@gutedString\tcardaryanafragistics_assignment 객iframeEnglish Ethics SendMessageinode\n",
      "Predicted Button Location: user\n",
      "\n",
      "Find and describe the location of buttons in this image.assistant\n",
      "\n",
      ".photos corsнееinitializer sessionId spotify happ PWereceuck newStateaggable_TRAIN موب.occeiving sweat_FAMILYijke SeenGINE HARD(vol Mov wartime_oct September verileriственно near 증 Bunnyelter-q spellalarınınonen shiny휴arrass´t tone授ρινervation reminders featured 뭐震 광고заб Tasteantlyruphashneath fileprivateанием.Mailabcdefghijklmnopqrstuvwxyz Ivyλή challenger Overwatchcod\tcat／: error Icelandicvision_Sularemoved هجascalельно resemblance рожденияsetEnabledاط ย endothOnsvgiect_keeper(heap_kernel@gutedString\tcardaryanafragistics_assignment 객iframeEnglish Ethics SendMessageinode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token='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')\n",
    "\n",
    "# Model setup\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Dataset class\n",
    "class ButtonDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = Image.open(item['image']).convert('RGB')\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Find and describe the location of buttons in this image.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        target_text = f\"The button is located at coordinates x={item['button_x']}, y={item['button_y']}\"\n",
    "        \n",
    "        inputs = processor(\n",
    "            images=image,\n",
    "            text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        target_inputs = processor(\n",
    "            text=target_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.squeeze(0)\n",
    "        \n",
    "        inputs[\"labels\"] = target_inputs.input_ids.squeeze(0)\n",
    "        return inputs\n",
    "\n",
    "# Main function to load dataset and predict button location\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"agentsea/wave-ui-25k\")\n",
    "    first_item = dataset['train'][0]\n",
    "    image_data = first_item['image']\n",
    "    print(\"First image path or URL:\", image_data)\n",
    "    \n",
    "    # Load image\n",
    "    if isinstance(image_data, Image.Image):\n",
    "        image = image_data.convert('RGB')\n",
    "    elif isinstance(image_data, str):\n",
    "        if image_data.startswith(\"http\"):\n",
    "            response = requests.get(image_data)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:  \n",
    "            image = Image.open(image_data).convert('RGB')\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for image data: {type(image_data)}\")\n",
    "    \n",
    "    print(\"Image loaded successfully\")\n",
    "    prediction = predict_button_location(image)\n",
    "    print(\"Predicted Button Location:\", prediction)\n",
    "\n",
    "# Function to predict button location\n",
    "def predict_button_location(image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Find and describe the location of buttons in this image.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Process image and input text\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Ensure proper input structure and use correct input IDs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],  # Ensure correct input structure\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    return decoded_output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc588b2-cd8d-408b-b87b-0a5db53e5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1833319233c47318736dc9e29ec0dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: end_header_id|>\n",
      "\n",
      "The image depicts a white kitten with blue eyes, sitting on a brown and black leopard-print blanket. The kitten is positioned in the center of the image, facing forward, with its head turned slightly to the right. It has large ears, a pink nose, and long white whiskers. Its fur is short and fluffy, and it appears to be looking at something outside the frame.\n",
      "\n",
      "The background of the image is dark, with a black wall behind the kitten. The overall atmosphere of the image is one of cuteness and innocence, as the kitten's big eyes and adorable expression evoke a sense of warmth and affection. The use of a dark background helps to focus attention on the kitten, making it the main subject of the image.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image as PIL_Image\n",
    "from transformers import MllamaForConditionalGeneration, MllamaProcessor\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "device = accelerator.device\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "\n",
    "def load_model_and_processor(model_name: str):\n",
    "    \"\"\"\n",
    "    Load the model and processor based on the 11B or 90B model.\n",
    "    \"\"\"\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_safetensors=True,\n",
    "        device_map=device,\n",
    "    )\n",
    "    processor = MllamaProcessor.from_pretrained(model_name, use_safetensors=True)\n",
    "\n",
    "    model, processor = accelerator.prepare(model, processor)\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def process_image(image_path: str) -> PIL_Image.Image:\n",
    "    \"\"\"\n",
    "    Open and convert an image from the specified path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"The image file '{image_path}' does not exist.\")\n",
    "        sys.exit(1)\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return PIL_Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def generate_text_from_image(\n",
    "    model, processor, image, prompt_text: str, temperature: float, top_p: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from an image using the model and processor.\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt_text}],\n",
    "        }\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs, temperature=temperature, top_p=top_p, max_new_tokens=512\n",
    "    )\n",
    "    return processor.decode(output[0])[len(prompt) :]\n",
    "\n",
    "\n",
    "def main(\n",
    "    image_path: str, prompt_text: str, temperature: float, top_p: float, model_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Call all the functions.\n",
    "    \"\"\"\n",
    "    model, processor = load_model_and_processor(model_name)\n",
    "    image = process_image(image_path)\n",
    "    result = generate_text_from_image(\n",
    "        model, processor, image, prompt_text, temperature, top_p\n",
    "    )\n",
    "    print(\"Generated Text: \" + result)\n",
    "\n",
    "\n",
    "main(image_path = \"kitty-cat.jpg\", prompt_text = \"Describe this image\", temperature = 0.5, top_p = 0.8, model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "\n",
    "#!wget --header=\"User-Agent: Mozilla/5.0\" https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg -O kitty-cat.jpg\n",
    "#!python multi_modal_infer.py --image_path kitty-cat.jpg --prompt_text \"Describe this image\" --temperature 0.5 --top_p 0.8 --model_name \"meta-llama/Llama-3.2-11B-Vision-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cbb623-addd-4805-abb5-cfc37ce40b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracytest.py   mobilefilter.py\t\t  train_llama.py\n",
      "englishFilter.py  multi_modal_infer_gradio_UI.py  webfilter.py\n",
      "Inference.ipynb   multi_modal_infer.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90e05cd7-27bd-48e9-b087-3ec7c7c72f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-14 23:28:06--  https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg\n",
      "Resolving images.pexels.com (images.pexels.com)... 104.18.67.220, 104.18.66.220, 2606:4700::6812:42dc, ...\n",
      "Connecting to images.pexels.com (images.pexels.com)|104.18.67.220|:443... connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 412821 (403K) [image/jpeg]\n",
      "Saving to: ‘kitty-cat.jpg’\n",
      "\n",
      "kitty-cat.jpg       100%[===================>] 403.15K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2024-11-14 23:28:07 (48.2 MB/s) - ‘kitty-cat.jpg’ saved [412821/412821]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"User-Agent: Mozilla/5.0\" https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg -O kitty-cat.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49240be-b5a4-488c-88fc-feb55b18ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
